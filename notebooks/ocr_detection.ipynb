{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OS, setting OS environment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALert! This cuda_launch_blocking is only for debugging purposes. It is not recommended to use it in production.\n",
    "# This will SLOW DOWN the training process.\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18, ResNet50_Weights, ResNet18_Weights\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import time\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Wandb to record the experiment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"OCR_Recognition\", name=\"ResNet18_LSTM_1\")\n",
    "\n",
    "wandb.config.update({\"starting_learning_rate\": 0.001, \"epochs\": 200, \"batch_size\": 32})\n",
    "wandb.config.update({\"cnn_backend\": \"ResNet50\", \"dataset\": \"OCR\", \"optimizer\": \"AdamW\", \"scheduler\": \"ReduceLROnPlateau\"})\n",
    "wandb.config.update({\"loss_function\": \"CTCLoss\", \"pretrained\": True, \"pretrained_weights\": \"IMAGENET1K_V2\"})\n",
    "wandb.config.update({\"lr_scheduler\": \"ReduceLROnPlateau\", \"lr_patience\": 5, \"lr_factor\": 0.1, \"lr_min\": 1e-6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Custom Tokenizer used for OCR Detection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Tokenizer for OCR Detection task\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file = 'C:/Users/ra78lof/occinference/byte-level-BPE.tokenizer.json')\n",
    "#feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/swin-base-batch4-window7-224-in22k')\n",
    "\n",
    "# Add PAD token to the vocabulary, otherwise it will throw an error\n",
    "tokenizer.add_special_tokens({'pad_token': \"pad_token\"})\n",
    "\n",
    "# Debug test for blank token, this token is required for CTC loss\n",
    "# tokenizer.decode(62)\n",
    "\n",
    "# Debug test for pad token, this token is required for padding sequences\n",
    "# print(tokenizer.pad_token_id)\n",
    "\n",
    "# Debug test for token length, this is required for the model building\n",
    "# print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CustomDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, excel_file, img_dir, tokenizer = tokenizer, feature_extractor = None, transform=None, max_target_length = 45):\n",
    "        self.data = pd.read_excel(excel_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transform\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data['ImageName'][idx]\n",
    "        text = self.data['Labels'][idx]\n",
    "        image = Image.open(img_name).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # The labels MUST be tokenized and padded\n",
    "        labels = self.tokenizer(text, padding = 'max_length', max_length = self.max_target_length).input_ids\n",
    "        \n",
    "\n",
    "        return image, torch.as_tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model architecture, this CNN-LSTM architecture includes the following part: A modified resnet18 with pretrained weights, some medium CNN layers and LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified ResNet architecture for Optical Character Recognition (OCR).\n",
    "\n",
    "    Attributes:\n",
    "        features (nn.Sequential): A sequential container of the original ResNet layers excluding avgpool and fc layers.\n",
    "        conv1 (nn.Conv2d): Convolution layer to adjust input channels to 1 (grayscale images).\n",
    "        post_resnet1 (nn.Conv2d): Convolution layer following the features layer.\n",
    "        bn1 (nn.BatchNorm2d): Batch normalization layer following post_resnet1.\n",
    "        relu1 (nn.ReLU): ReLU activation layer following bn1.\n",
    "        post_resnet2 (nn.Conv2d): Another convolution layer following relu1.\n",
    "        bn2 (nn.BatchNorm2d): Batch normalization layer following post_resnet2.\n",
    "        relu2 (nn.ReLU): ReLU activation layer following bn2.\n",
    "        post_resnet3 (nn.Conv2d): Another convolution layer following relu2.\n",
    "        bn3 (nn.BatchNorm2d): Batch normalization layer following post_resnet3.\n",
    "        relu3 (nn.ReLU): ReLU activation layer following bn3.\n",
    "        dwv (nn.Conv2d): Depthwise convolution layer for channel reduction following relu3.\n",
    "        lstm1 (nn.LSTM): LSTM layer following the depthwise convolution.\n",
    "        linear1 (nn.Linear): Fully connected layer to project LSTM output to class scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, original_resnet: nn.Module):\n",
    "        \"\"\"\n",
    "        Initializes the ModifiedResNet with an original_resnet model.\n",
    "\n",
    "        Args:\n",
    "            original_resnet (nn.Module): The original ResNet model.\n",
    "        \"\"\"\n",
    "        super(ModifiedResNet, self).__init__()\n",
    "        self.features = nn.Sequential(*list(original_resnet.children())[:-2]) # Remove avgpool and fc layers in the original resnet\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) # Adjust input channels to 1, since the input images are grayscale\n",
    "        \n",
    "        self.post_resnet1 = nn.Conv2d(512, 512, kernel_size=(2, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(512) # Batch normalization after post_resnet1, important for training\n",
    "        self.relu1 = nn.ReLU(inplace=True) # ReLU activation after post_resnet1, import for training\n",
    "        \n",
    "        self.post_resnet2 = nn.Conv2d(512, 512, kernel_size=(3, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(512) # Batch normalization after post_resnet2, important for training\n",
    "        self.relu2 = nn.ReLU(inplace=True) # ReLU activation after post_resnet2, import for training\n",
    "        \n",
    "        self.post_resnet3 = nn.Conv2d(512, 512, kernel_size=(2, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(512) # Batch normalization after post_resnet3, important for training\n",
    "        self.relu3 = nn.ReLU(inplace=True) # ReLU activation after post_resnet3, import for training\n",
    "        \n",
    "        self.dwv = nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False) # Depthwise Convolution for channel reduction\n",
    "        self.lstm1 = nn.LSTM(bidirectional=True, num_layers=2, input_size=128, hidden_size=128, dropout=0)\n",
    "        self.linear1 = nn.Linear(256, 82) # Project first dimension of LSTM output to 82 (number of classes including the PAD token)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the ModifiedResNet.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.post_resnet1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.post_resnet2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.post_resnet3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        x = self.dwv(x)\n",
    "        \n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.permute(0, 2, 3, 1).contiguous() # Change the order of the dimensions, this is required for the LSTM layer\n",
    "        x = x.view(batch_size, height * width, channels) # Reshape to (batch_size, sequence_length, input_dim)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.linear1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple debug test with a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 45, 82])\n"
     ]
    }
   ],
   "source": [
    "# Debug test for the ModifiedResNet\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dummy_input = torch.randn(32, 1, 500, 1200).to(device)\n",
    "\n",
    "original_resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "original_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "model= ModifiedResNet(original_resnet).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "    print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation for Train, valid and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for the dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((500, 1200)),  # Resize images to the required dimensions\n",
    "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training, Validation and Test data\n",
    "\n",
    "train_dataset = CustomDataset(excel_file='C:/Users/ra78lof/occinference/Test_data.xlsx',\n",
    "                             img_dir='C:/Users/ra78lof/occinference/Test_data/', tokenizer = tokenizer, transform=transform)\n",
    "\n",
    "valid_dataset = CustomDataset(excel_file='C:/Users/LMMISTA-WAP265/OcciGen/data/dom_project/Val_data.xlsx',\n",
    "                              img_dir='C:/Users/LMMISTA-WAP265/OcciGen/data/dom_project/Val_data/', tokenizer = tokenizer, transform=transform)\n",
    "\n",
    "test_dataset = CustomDataset(excel_file='C:/Users/ra78lof/occinference/Test_data.xlsx',\n",
    "                             img_dir='C:/Users/ra78lof/occinference/Test_data/', tokenizer = tokenizer, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, optimizer, scheduler and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, optimizer, scheduler and loss function\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "original_resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "original_resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model= ModifiedResNet(original_resnet).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 10, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "# The following scheduler can be used during validation\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, threshold=0.0001, min_lr=0.00001)\n",
    "# ...valid_loss += loss.item()\n",
    "#    scheduler.step(valid_loss)...\n",
    "\n",
    "# The CTC loss function is returning blank predictions for some reason\n",
    "# criterion = CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Wandb to track the Epochs\n",
    "# wandb.watch(model, log=\"all\")\n",
    "epochs = 200\n",
    "\n",
    "def train(model, train_loader, optimizer, epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) \n",
    "\n",
    "        loss = criterion(output.view(-1, output.size(2)), target.view(-1))  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # wandb.log({'Train Loss': train_loss / (batch_idx + 1)})\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n",
    "                epochs, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       train_loss / (batch_idx + 1)))\n",
    "    scheduler.step()       \n",
    "    end_time = time.time()\n",
    "    print(\"Time taken for epoch: \", end_time - start_time)\n",
    "    return train_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(valid_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "           \n",
    "            loss = criterion(output.view(-1, output.size(2)), target.view(-1))\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            # scheduler.step(valid_loss)\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "    wandb.log({'Validation Loss': valid_loss})\n",
    "    print('\\nValidation set: Average loss: {:.4f}\\n'.format(valid_loss))\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.load_state_dict(torch.load('C:/Users/ra78lof/occinference/ocr_model.pt'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "        \n",
    "\n",
    "            #tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            loss = criterion(output.view(-1, output.size(2)), target.view(-1))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    wandb.log({'Test Loss': test_loss})\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the whole training, validating and test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(training_epochs, save_dir, model_save_name):\n",
    "    best_loss = float('inf')\n",
    "    # Lists to keep track of losses over epochs\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(1, training_epochs + 1):\n",
    "        # Capture train loss\n",
    "        train_loss = train(model, train_loader, optimizer, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Capture validation loss\n",
    "        valid_loss = validation(model, valid_loader)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            model_save_path = os.path.join(save_dir, model_save_name)\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'Model saved at epoch {epoch} with validation loss: {valid_loss:.6f}')\n",
    "        \n",
    "        # Capture test loss\n",
    "        test_loss = test(model, test_loader, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f'Test Loss at epoch {epoch}: {test_loss:.6f}')\n",
    "        print(f'Epoch {epoch}/{training_epochs}, Best Loss: {best_loss:.6f}\\n')\n",
    "\n",
    "    # Visualize the losses over epochs\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs, valid_losses, label='Validation Loss', marker='o')\n",
    "    plt.plot(epochs, test_losses, label='Test Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call your main function\n",
    "main(200, 'save_directory', 'model_name.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is setted up, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "training_epochs = 200\n",
    "save_dir = 'C:/Users/ra78lof/occinference'\n",
    "model_save_name = 'ocr_model.pt'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= ModifiedResNet(original_resnet).to(device)\n",
    "\n",
    "# Load our trained model\n",
    "model.load_state_dict(torch.load('C:/Users/ra78lof/occinference/ocr_model_10.22_9.pt'))\n",
    "# Call main function\n",
    "main(200, 'save_directory', 'model_name.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017139\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.020393\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.019644\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.019518\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.018999\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.019054\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.019177\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.019373\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.019486\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.019460\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.019171\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.019036\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.018900\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.019431\n",
      "Time taken for epoch:  177.0628731250763\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017264\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017849\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.018141\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017465\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017514\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017705\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017911\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.018743\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.018737\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.018520\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.018515\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.018565\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.018487\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.018684\n",
      "Time taken for epoch:  176.05991744995117\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.016228\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017506\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.018227\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017793\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017837\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017724\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017324\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017316\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.017566\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.017702\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.017592\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.017571\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.017435\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.019437\n",
      "Time taken for epoch:  176.69951248168945\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017527\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.020809\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.019108\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.019207\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.018971\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.019344\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.018612\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.018218\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.018319\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.018165\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.017932\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.018065\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.017929\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.017874\n",
      "Time taken for epoch:  176.2782781124115\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.010029\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015181\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015909\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016852\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016717\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016815\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016857\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016759\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016853\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016971\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016904\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016971\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016963\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.019200\n",
      "Time taken for epoch:  176.85756039619446\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.022687\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016885\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.017848\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017347\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017440\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017510\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017282\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017311\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.017134\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.017176\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.017359\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.017412\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.017292\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.017321\n",
      "Time taken for epoch:  176.91308784484863\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.023558\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016391\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016081\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017105\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017122\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016800\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016488\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016355\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016640\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016617\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016357\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016325\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016520\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016831\n",
      "Time taken for epoch:  177.16970205307007\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.020877\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016928\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016179\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015876\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016437\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016957\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016915\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016724\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016286\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016416\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016308\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016380\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016666\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016667\n",
      "Time taken for epoch:  176.50205731391907\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.018880\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014823\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016948\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017375\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017203\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017147\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017049\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016914\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016691\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016757\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016804\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016558\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016510\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016819\n",
      "Time taken for epoch:  188.41478276252747\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.019594\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015670\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015122\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016216\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016323\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016611\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016417\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016708\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016406\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016402\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016396\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016420\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016424\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016509\n",
      "Time taken for epoch:  211.09990048408508\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017725\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.018239\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016886\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015582\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014888\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014875\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015438\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015801\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016484\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016299\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016650\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016616\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016646\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016583\n",
      "Time taken for epoch:  210.6844358444214\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008793\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016784\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.017299\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016341\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016446\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016071\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016706\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017057\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016836\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016781\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016724\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016579\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016619\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016645\n",
      "Time taken for epoch:  205.44177412986755\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014746\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015624\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016777\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016093\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016333\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015960\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016573\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016326\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016221\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016155\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016333\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016363\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016454\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016480\n",
      "Time taken for epoch:  210.15373754501343\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008855\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014094\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015795\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016098\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016045\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016709\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016316\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016431\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016435\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016564\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016461\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016282\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016352\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.017380\n",
      "Time taken for epoch:  210.32260370254517\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.026446\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.018896\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.020788\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.020525\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.019669\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.019290\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.018916\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.018532\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.018877\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.018499\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.018255\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.018233\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.018341\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.017996\n",
      "Time taken for epoch:  210.1802523136139\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.016154\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017274\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015073\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015990\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016092\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016458\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016556\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016854\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016592\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016481\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016628\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016717\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016704\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016721\n",
      "Time taken for epoch:  210.14498233795166\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014593\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.018493\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.017497\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017035\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017230\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016252\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016553\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016796\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.017218\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.017077\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016990\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016899\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016966\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016880\n",
      "Time taken for epoch:  210.13938093185425\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.015650\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014743\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015974\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016190\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016501\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016591\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016558\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016433\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016235\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016041\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016029\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016022\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016289\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016534\n",
      "Time taken for epoch:  207.47882270812988\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.024733\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017467\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016120\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016032\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015718\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015392\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015403\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015663\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015889\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015809\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015930\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016228\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016197\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016325\n",
      "Time taken for epoch:  175.71558141708374\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.013300\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015496\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016548\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016239\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016493\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016650\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016396\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016137\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016110\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015883\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016138\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016080\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016289\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016534\n",
      "Time taken for epoch:  175.3055739402771\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.016242\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015317\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015757\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015142\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015671\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015939\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016244\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015757\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015648\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015639\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015795\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015937\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016096\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016401\n",
      "Time taken for epoch:  175.74551033973694\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.015630\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017739\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016849\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017345\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.018215\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.018054\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017588\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017103\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016959\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016747\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016671\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016342\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016206\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016414\n",
      "Time taken for epoch:  176.02959942817688\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008889\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015530\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.017603\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016855\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016547\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016896\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016337\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016091\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016385\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016300\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016275\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016174\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015937\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016033\n",
      "Time taken for epoch:  175.6325318813324\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.018673\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015396\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015118\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015986\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015646\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015245\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015216\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015182\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015168\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015342\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015271\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015134\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015173\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015187\n",
      "Time taken for epoch:  175.32667112350464\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.018347\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.011765\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014218\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014994\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014878\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014697\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014408\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014278\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014407\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014541\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014614\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014635\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014713\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014758\n",
      "Time taken for epoch:  175.4162266254425\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.013185\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014389\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013886\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013097\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013886\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014199\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014109\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014350\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014382\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014379\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014505\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014570\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014667\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015133\n",
      "Time taken for epoch:  175.42485976219177\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017521\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015444\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014760\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014851\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014362\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014542\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014753\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014555\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014086\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014306\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014223\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014233\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014414\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014610\n",
      "Time taken for epoch:  175.42849946022034\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014396\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.018194\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016572\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015395\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014895\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014587\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014640\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014528\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014717\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014777\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014800\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014740\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014522\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014469\n",
      "Time taken for epoch:  175.33762097358704\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.011549\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014285\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014758\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014936\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014864\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014593\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014536\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014496\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014527\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014476\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014381\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014393\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014450\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014479\n",
      "Time taken for epoch:  175.65982174873352\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.015817\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015696\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016227\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015297\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014610\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014693\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015163\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015038\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014947\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014851\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014736\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014545\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014599\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014454\n",
      "Time taken for epoch:  175.6221661567688\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.012696\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014471\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013909\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014207\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013918\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014140\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014261\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014402\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014942\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014738\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014555\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014481\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014433\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014327\n",
      "Time taken for epoch:  175.24019241333008\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.016179\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014123\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013598\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013472\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013878\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014171\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014381\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014567\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014340\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014348\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014260\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014295\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014412\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014321\n",
      "Time taken for epoch:  175.38169050216675\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014498\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014645\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014938\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015011\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014678\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014478\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014410\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014220\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014089\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013909\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013987\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014243\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014214\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014289\n",
      "Time taken for epoch:  177.1277506351471\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.007077\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013770\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013962\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014216\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014352\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014275\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014267\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014356\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014351\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014421\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014412\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014267\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014360\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014432\n",
      "Time taken for epoch:  186.16113924980164\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.013446\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014835\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015483\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015770\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015706\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015091\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014653\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014712\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015060\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014862\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014726\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014571\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014579\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014563\n",
      "Time taken for epoch:  213.72845005989075\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014561\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013882\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014075\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014804\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014760\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014583\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014532\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014720\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014652\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014751\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014760\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014681\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014748\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014889\n",
      "Time taken for epoch:  212.54087567329407\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.015293\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017718\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.017556\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016997\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016527\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015992\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015772\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015803\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015779\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015480\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015494\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015591\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015380\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015655\n",
      "Time taken for epoch:  214.0865547657013\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017299\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017859\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.017971\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017655\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.018205\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017861\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017421\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017277\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.017002\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.017145\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.017168\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.017117\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016952\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016936\n",
      "Time taken for epoch:  214.0710892677307\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017759\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.020796\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.019130\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.017369\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017316\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016985\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017617\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017343\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.017290\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.017178\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.017452\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.017352\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.017250\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.017226\n",
      "Time taken for epoch:  214.10678935050964\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.019444\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.018361\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.019046\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.018147\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.017672\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017671\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017071\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017253\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.017199\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.017136\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.017239\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.017092\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016964\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016824\n",
      "Time taken for epoch:  214.2035150527954\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.012877\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016898\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015691\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016000\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016384\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016244\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.016243\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.016216\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016342\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016048\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016027\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016053\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016018\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016488\n",
      "Time taken for epoch:  214.02564120292664\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014224\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.018557\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.020224\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.018883\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.018235\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.017532\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.017398\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.017167\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.016861\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.016641\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.016525\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.016681\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.016448\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016854\n",
      "Time taken for epoch:  212.3932557106018\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008480\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014892\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015643\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015623\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015617\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015836\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015751\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015414\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015517\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015485\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015537\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015595\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015572\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.016029\n",
      "Time taken for epoch:  212.38463759422302\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.019209\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017424\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016459\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.016560\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.016156\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.016239\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015547\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015446\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015494\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015252\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014998\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015062\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015091\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015063\n",
      "Time taken for epoch:  214.3496708869934\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.011998\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013725\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013557\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013489\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014081\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014583\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014453\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014639\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014522\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014654\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014507\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014425\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014540\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014338\n",
      "Time taken for epoch:  213.84030079841614\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.011571\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.012557\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.012712\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013565\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013519\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013968\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013549\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013758\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013868\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013723\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013717\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013961\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014025\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014604\n",
      "Time taken for epoch:  214.01664280891418\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014662\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014860\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013665\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014203\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013285\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013161\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013317\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013250\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013388\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013321\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013414\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013556\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013655\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013873\n",
      "Time taken for epoch:  214.0880947113037\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.013008\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016954\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015193\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014433\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014616\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014661\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014696\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014525\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014213\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014029\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014014\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013896\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013932\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013858\n",
      "Time taken for epoch:  214.1615047454834\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.015403\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.016075\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015391\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014619\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014069\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014022\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013557\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013704\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013691\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013920\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013826\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013892\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013782\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013795\n",
      "Time taken for epoch:  213.7439088821411\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014697\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013202\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014011\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013961\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013475\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013537\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013287\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013216\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013384\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013463\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013448\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013303\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013483\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014531\n",
      "Time taken for epoch:  214.02484941482544\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008225\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013054\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013989\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014036\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014024\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014057\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014048\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013829\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013854\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013886\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013589\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013844\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013789\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014265\n",
      "Time taken for epoch:  214.39866995811462\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.019150\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013526\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013924\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.012952\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014038\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013638\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013964\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013952\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013907\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014019\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013732\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013749\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013755\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013764\n",
      "Time taken for epoch:  214.17580389976501\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.020205\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014581\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014709\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014100\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014219\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014044\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013814\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014324\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014354\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014308\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014067\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013952\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013653\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013583\n",
      "Time taken for epoch:  214.0764036178589\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017104\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015559\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014297\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014218\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013898\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013793\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014126\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014114\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013831\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013660\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013630\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013513\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013549\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013658\n",
      "Time taken for epoch:  213.9234459400177\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.010387\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015323\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013957\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014060\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013626\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013819\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013801\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013780\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013914\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013885\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013513\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013672\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013691\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013703\n",
      "Time taken for epoch:  214.21514630317688\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008754\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013265\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013570\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013489\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013430\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013630\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.013545\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.013558\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.013232\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.013593\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.013675\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.013855\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.013921\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.013920\n",
      "Time taken for epoch:  213.693993806839\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.011359\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.013525\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013678\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.013514\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.013899\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014068\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014357\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014131\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014192\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014118\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014186\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014396\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014486\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014432\n",
      "Time taken for epoch:  214.00841188430786\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.016038\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.015156\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014305\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014844\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015015\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015680\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015719\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015530\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015326\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014985\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014789\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014747\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014510\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014525\n",
      "Time taken for epoch:  214.19455695152283\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.025540\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014821\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.015732\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015098\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015562\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015643\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015574\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015747\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015707\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015571\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015399\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015358\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015242\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015086\n",
      "Time taken for epoch:  213.10268807411194\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.014532\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.014096\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.014554\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014872\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014703\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.014593\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014839\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.015053\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.015091\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.015313\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015168\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014997\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014961\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015142\n",
      "Time taken for epoch:  212.36177158355713\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.017006\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.017041\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.016582\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.015910\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.015636\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.015606\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.015225\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014798\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014967\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014875\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.015018\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.015100\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.015143\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.015225\n",
      "Time taken for epoch:  183.54737186431885\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.008183\n",
      "Train Epoch: 200 [320/4163 (8%)]\tAverage loss: 0.012446\n",
      "Train Epoch: 200 [640/4163 (15%)]\tAverage loss: 0.013971\n",
      "Train Epoch: 200 [960/4163 (23%)]\tAverage loss: 0.014246\n",
      "Train Epoch: 200 [1280/4163 (31%)]\tAverage loss: 0.014016\n",
      "Train Epoch: 200 [1600/4163 (38%)]\tAverage loss: 0.013761\n",
      "Train Epoch: 200 [1920/4163 (46%)]\tAverage loss: 0.014617\n",
      "Train Epoch: 200 [2240/4163 (53%)]\tAverage loss: 0.014323\n",
      "Train Epoch: 200 [2560/4163 (61%)]\tAverage loss: 0.014378\n",
      "Train Epoch: 200 [2880/4163 (69%)]\tAverage loss: 0.014439\n",
      "Train Epoch: 200 [3200/4163 (76%)]\tAverage loss: 0.014394\n",
      "Train Epoch: 200 [3520/4163 (84%)]\tAverage loss: 0.014579\n",
      "Train Epoch: 200 [3840/4163 (92%)]\tAverage loss: 0.014469\n",
      "Train Epoch: 200 [390/4163 (99%)]\tAverage loss: 0.014632\n",
      "Time taken for epoch:  176.69791650772095\n",
      "Model Saved\n",
      "Train Epoch: 200 [0/4163 (0%)]\tAverage loss: 0.013945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ra78lof\\occinference\\beta_version.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m200\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_dir, \u001b[39m'\u001b[39m\u001b[39mocr_model_10.22_nico.pt\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mModel Saved\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ra78lof\\occinference\\beta_version.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# wandb.log({'Train Loss': train_loss / (batch_idx + 1)})\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ra78lof/occinference/beta_version.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 200):\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'ocr_model_10.22_nico.pt'))\n",
    "    print('Model Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode the Inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the test dataset to get the predictions\n",
    "label_list = []\n",
    "pred_list = []\n",
    "\n",
    "model= ModifiedResNet(original_resnet).to(device)\n",
    "\n",
    "# Load our trained model\n",
    "model.load_state_dict(torch.load('C:/Users/ra78lof/occinference/ocr_model_10.22_nico.pt'))\n",
    "\n",
    "for _, (data, target) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        output = model(data)\n",
    "        # Get the index of the class with the highest probability score\n",
    "        pred = output.softmax(dim=2)\n",
    "        pred = torch.argmax(pred, dim=2)\n",
    "        label_list += tokenizer.batch_decode(target, skip_special_tokens=True)\n",
    "        # label_list = label_list + tokenizer.batch_decode(target, skip_special_tokens=True)\n",
    "        pred_list += tokenizer.batch_decode(pred, skip_special_tokens=True)\n",
    "        #print(f'Current length label list: {len(label_list)}')\n",
    "        #print(f'Current length pred list: {len(pred_list)}')\n",
    "        #print(f'Label: {tokenizer.batch_decode(target, skip_special_tokens=True)}')\n",
    "        #print(f'Prediction: {tokenizer.batch_decode(pred, skip_special_tokens=True)}')\n",
    "        #print(f'Final length label list: {len(label_list)}')\n",
    "        #print(f'Final length pred list: {len(pred_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ['SEQUA@SECA', 'BLANQUINEU@BLANQUINEL', 'SEMMANA@SEMANA', 'BARRIU@BARRIL', 'BATISME@BAPTISME', 'ANIC@AMIC', 'MIRAC@MIRAT', 'DESCRIEURE@DESCRIURE', 'EMBOCHAR@EMBOCAR', 'POYSANSA@POISANSA', 'BASTARDO@BASTARDA', 'BENDENHA@VENDEMIA', 'MINIEYRA@MENIERA', 'SORTY@SORTIR', 'MEYNADES@MAINADA', 'FOPELANDA@OPALANDA', 'MESTURE@MESTURA', 'RECEPTATION@RECEPTACION', 'DAROCAR@DEROCAR', 'MELHUIRAR@MELHORAR', 'MARTYRIAR@MARTIRIAR', 'CARAMELAR@CALAMELAR', 'FILLOL@FILHOL', 'ASO@AZON', 'FOGASSE@FOGASA', 'CAMBA@CAMBE', 'PATEYAR@PATIAR', 'APENDRE@APRENDRE', 'CABIROUS@CABRION', 'FORASTEYR@FORESTIER', 'FRAGURA@FRACHURA', 'MERSÉS@MERCE']\n",
      "Prediction: ['SEQUA@SEA', 'BLANQUINEU@BLANQUINEL', 'SEMMANA@SEMANA', 'BARRIU@BARRIL', 'BATISME@BAPTISME', 'ANIC@AMIC', 'MIRAC@MIRAT', 'DESCRIEURE@DESCRIURE', 'EMBOCHAR@EMBOCAR', 'POYSANSA@POISANSA', 'BASTARDO@BASTARDA', 'BENDENHA@VENDEMIA', 'MINIEYRA@MENIERA', 'SORTY@SORTIR', 'MEYNADES@MAINADA', 'FOPELANSA@OPALANDA', 'MESTURE@MESTURA', 'RECEPTATION@RECEPTACION', 'DAROCAR@DEROCAR', 'MELHUIRAR@MELHORAR', 'MARTYRIAR@MARTIRIAR', 'CARAMELAR@CALAMELAR', 'FILLOL@FILHOL', 'ASO@AZON', 'FOGASSE@FOGASA', 'CAMBA@CAGBE', 'PATEYAR@PATIAR', 'APENDRE@APRENDRE', 'CABIROUS@CABRION', 'FORASTEYR@FORESTIER', 'FRAGURA@FRACHURA', 'MERSÉS@MERCE']\n"
     ]
    }
   ],
   "source": [
    "# Decode one batch of the test dataset to get the predictions\n",
    "# This is just for debugging purposes  \n",
    "'''\n",
    "data, target = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "target = target.to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(data)\n",
    "pred = output.softmax(dim=2)\n",
    "pred = torch.argmax(pred, dim=2)\n",
    "\n",
    "# Decode the predictions and labels\n",
    "label = tokenizer.batch_decode(target, skip_special_tokens=True)\n",
    "pred = tokenizer.batch_decode(pred, skip_special_tokens=True)\n",
    "\n",
    "# Print the predictions and labels\n",
    "print(f'Label: {label}')\n",
    "print(f'Prediction: {pred}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predictions in an excel file\n",
    "\n",
    "pd.DataFrame({'label': label_list, 'pred': pred_list}).to_excel('C:/Users/ra78lof/occinference/ocr_predictions_10.22_10.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
